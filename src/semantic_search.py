"""
Phase 4: Recherche sÃ©mantique et analyse avancÃ©e
"""

from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from typing import List, Dict
import numpy as np
import logging

logger = logging.getLogger(__name__)

class SemanticSearchEngine:
    def __init__(self, model_name='sentence-transformers/all-MiniLM-L6-v2'):
        """Initialise le moteur de recherche sÃ©mantique"""
        from src.database import VectorDatabase
        
        self.db = VectorDatabase()
        self.model = SentenceTransformer(model_name)
        if not self.db.connect():
            raise ConnectionError("Impossible de se connecter Ã  la base de donnÃ©es")
    
    def search_by_error(self, error_description: str, top_k: int = 10) -> List[Dict]:
        """Cas d'usage 1: Retrouver tous les logs similaires Ã  une erreur donnÃ©e"""
        print(f"\nðŸ” Recherche sÃ©mantique pour: '{error_description}'")
        
        query_embedding = self.model.encode(error_description)
        results = self.db.semantic_search(query_embedding.tolist(), top_k=top_k, threshold=0.3)
        
        print(f"âœ“ {len(results)} logs similaires trouvÃ©s:")
        for i, result in enumerate(results[:5], 1):
            print(f"\n  {i}. [SimilaritÃ©: {result['similarity']:.2%}]")
            print(f"     Level: {result['log_level']}")
            print(f"     Texte: {result['text'][:100]}...")
        
        return results
    
    def find_error_clusters(self, n_clusters: int = 5) -> Dict:
        """Cas d'usage 2: Identifier les groupes d'erreurs frÃ©quentes"""
        print(f"\nðŸ‘¥ Clustering des erreurs (k={n_clusters})...")
        
        query = """
        SELECT l.id, le.embedding, l.original_text, l.log_level
        FROM log_embeddings le
        JOIN logs l ON le.log_id = l.id
        WHERE l.log_level IN ('ERROR', 'CRITICAL', 'FATAL')
        LIMIT 5000
        """
        
        try:
            self.db.cursor.execute(query)
            results = self.db.cursor.fetchall()
            
            if not results or len(results) < n_clusters:
                logger.warning("Pas assez de logs d'erreur pour le clustering")
                return {}
            
            log_ids = [row[0] for row in results]
            # Parser le format vector de PostgreSQL
            embeddings = []
            for row in results:
                emb_val = row[1]
                try:
                    # cas: embedding stockÃ© en BYTEA (pickle)
                    import pickle
                    if isinstance(emb_val, (bytes, bytearray, memoryview)):
                        b = bytes(emb_val)
                        emb = pickle.loads(b)
                        embeddings.append(np.array(emb, dtype=float))
                    # cas: embedding stockÃ© en texte '[0.1, 0.2, ...]'
                    elif isinstance(emb_val, str):
                        s = emb_val.strip('[]')
                        if s:
                            emb = [float(x) for x in s.split(',') if x.strip()]
                            embeddings.append(np.array(emb, dtype=float))
                    else:
                        # type inconnu, tenter de convertir
                        try:
                            emb = np.array(emb_val, dtype=float)
                            if emb.ndim == 1:
                                embeddings.append(emb)
                        except Exception:
                            continue
                except Exception:
                    # ignorer les embeddings malformÃ©s
                    continue
            
            if not embeddings:
                logger.warning("Aucun embedding valide pour le clustering")
                return {}

            embeddings = np.vstack(embeddings)
            if embeddings.ndim != 2 or embeddings.shape[0] < 1:
                logger.warning("Embeddings invalides pour le clustering")
                return {}

            n_clusters_use = min(n_clusters, embeddings.shape[0])
            try:
                kmeans = KMeans(n_clusters=n_clusters_use, random_state=42, n_init=10)
                labels = kmeans.fit_predict(embeddings)
            except Exception as e:
                logger.error(f"Erreur KMeans: {e}")
                return {}
            
            clusters = {}
            for cluster_id in range(len(set(labels))):
                mask = labels == cluster_id
                cluster_logs = [log_ids[i] for i, m in enumerate(mask) if m]
                clusters[cluster_id] = {
                    'size': len(cluster_logs),
                    'centroid': kmeans.cluster_centers_[cluster_id].tolist()[:5],
                    'sample_logs': cluster_logs[:3]
                }
            
            print(f"\nâœ“ {len(clusters)} clusters identifiÃ©s:")
            for cluster_id, info in clusters.items():
                print(f"  Cluster {cluster_id}: {info['size']} logs")
            
            return clusters
            
        except Exception as e:
            logger.error(f"Erreur clustering: {e}")
            return {}
    
    def analyze_temporal_evolution(self, error_pattern: str, days: int = 7) -> Dict:
        """Cas d'usage 3: Analyser l'Ã©volution temporelle des erreurs similaires"""
        print(f"\nðŸ“… Analyse temporelle pour: '{error_pattern}'")
        
        query_embedding = self.model.encode(error_pattern)
        results = self.db.semantic_search(query_embedding.tolist(), top_k=1000, threshold=0.4)
        
        if not results:
            logger.warning("Aucun log trouvÃ© pour ce pattern")
            return {}
        
        # Grouper par date
        from collections import defaultdict
        temporal_data = defaultdict(int)
        
        for result in results:
            timestamp = result.get('timestamp')
            if timestamp:
                date_key = str(timestamp)[:10]  # YYYY-MM-DD
                temporal_data[date_key] += 1
        
        sorted_data = dict(sorted(temporal_data.items()))
        
        print(f"\nâœ“ Distribution temporelle ({len(sorted_data)} jours):")
        for date, count in list(sorted_data.items())[:10]:
            print(f"  {date}: {count} erreurs")
        
        return sorted_data
    
    def search_by_keyword(self, query: str, top_k: int = 10) -> List[Dict]:
        """Cas d'usage 4 (partie): Recherche simple par mots-clÃ©s"""
        print(f"\nðŸ”Ž Recherche par mot-clÃ©: '{query}'")
        
        keyword_query = f"%{query.lower()}%"
        self.db.cursor.execute("""
            SELECT id, original_text, log_level, timestamp
            FROM logs
            WHERE normalized_text LIKE %s
            LIMIT %s
        """, (keyword_query, top_k))
        
        keyword_results = []
        for row in self.db.cursor.fetchall():
            keyword_results.append({
                'id': row[0],
                'text': row[1],
                'log_level': row[2],
                'timestamp': row[3],
                'similarity': 1.0  # Mot-clÃ© trouvÃ© = 100% de match
            })
        
        print(f"âœ“ {len(keyword_results)} rÃ©sultats trouvÃ©s par mot-clÃ©")
        return keyword_results
    
    def compare_with_keyword_search(self, query: str, top_k: int = 10) -> Dict:
        """Compare recherche sÃ©mantique vs recherche par mots-clÃ©s"""
        print(f"\nðŸ”¬ Comparaison: SÃ©mantique vs Mot-clÃ©")
        print(f"   RequÃªte: '{query}'")
        
        query_embedding = self.model.encode(query)
        semantic_results = self.db.semantic_search(query_embedding.tolist(), top_k=top_k)
        
        # Recherche par mot-clÃ© simple
        keyword_query = f"%{query.lower()}%"
        self.db.cursor.execute("""
            SELECT id, original_text, log_level, timestamp
            FROM logs
            WHERE normalized_text LIKE %s
            LIMIT %s
        """, (keyword_query, top_k))
        
        keyword_results = []
        for row in self.db.cursor.fetchall():
            keyword_results.append({
                'id': row[0],
                'text': row[1],
                'log_level': row[2],
                'timestamp': row[3]
            })
        
        print(f"\nâœ“ RÃ©sultats:")
        print(f"  Recherche sÃ©mantique: {len(semantic_results)} rÃ©sultats")
        print(f"  Recherche par mot-clÃ©: {len(keyword_results)} rÃ©sultats")
        
        return {
            'semantic': semantic_results,
            'keyword': keyword_results,
            'semantic_count': len(semantic_results),
            'keyword_count': len(keyword_results)
        }
    
    def close(self):
        """Ferme la connexion Ã  la base"""
        self.db.disconnect()


def run_demo():
    """DÃ©mo des cas d'usage"""
    
    engine = SemanticSearchEngine()
    
    print("\n" + "="*70)
    print("ðŸš€ DÃ‰MONSTRATION - RECHERCHE SÃ‰MANTIQUE")
    print("="*70)
    
    try:
        print("\n" + "-"*70)
        print("ðŸ“Œ CAS 1: Retrouver logs similaires Ã  une erreur donnÃ©e")
        print("-"*70)
        engine.search_by_error("Database connection timeout error", top_k=5)
        
        print("\n" + "-"*70)
        print("ðŸ“Œ CAS 2: Identifier les groupes d'erreurs frÃ©quentes")
        print("-"*70)
        clusters = engine.find_error_clusters(n_clusters=3)
        
        print("\n" + "-"*70)
        print("ðŸ“Œ CAS 3: Analyser l'Ã©volution des erreurs")
        print("-"*70)
        engine.analyze_temporal_evolution("connection error", days=7)
        
        print("\n" + "-"*70)
        print("ðŸ“Œ CAS 4: Comparer recherche sÃ©mantique vs mot-clÃ©")
        print("-"*70)
        engine.compare_with_keyword_search("timeout", top_k=5)
        
    finally:
        engine.close()


if __name__ == "__main__":
    run_demo()